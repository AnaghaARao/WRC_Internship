{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ycebjQpWJ3u9"
      },
      "outputs": [],
      "source": [
        "def classify(url):\n",
        "  !pip install cloudscraper\n",
        "  !pip install beautifulsoup4\n",
        "  !pip install --upgrade google-cloud-language\n",
        "\n",
        "  import cloudscraper\n",
        "\n",
        "  from bs4 import BeautifulSoup\n",
        "  scraper = cloudscraper.create_scraper()\n",
        "  headers = {'user-agent' : 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121'}\n",
        "\n",
        "  classifiers = {\"Adult\" : 10 , \"Arts & Entertainment\" : 12 , \"Autos & Vehicles\" : 2 , \"Beauty & Fitness\" : 15 , \"Books & Literature\" : 12 , \"Business & Industrial\" : 1 , \"Computer and Electronics\" : 18 , \"Finance\" : 18 , \"Food & Drink\" : 20 , \"Games\" : 12 , \"Health\" : 20 , \"Hobbies & Leisure\" : 10 , \"Home & Garden\" : 12,\"Internet & Telecom\" : 12,\"Jobs & Education\" : 15,\"Law & Government\" : 10,\"News\" : 8,\"Online Communities\" : 12,\"People & Society\" : 10,\"Pets & Animals\" : 15,\"Real Estate\" : 20,\"Reference\" : 12,\"Science\" : 12,\"Sensitive Subjects\" : 12,\"Shopping\" : 22,\"Sports\" : 10,\"Travel & Transportation\" : 12,\"others\" : 8, 'none': 0}\n",
        "\n",
        "  try:\n",
        "      r = scraper.get(url, headers = headers)\n",
        "\n",
        "      soup = BeautifulSoup(r.text , 'html.parser')\n",
        "      title = soup.find('title').text\n",
        "      description = soup.find('meta', attrs={'name' : 'description'})\n",
        "\n",
        "      if \"content\" in str(description):\n",
        "        description = description.get(\"content\")\n",
        "      else:\n",
        "        description = \"\"\n",
        "\n",
        "      h1 = soup.find_all('h1')\n",
        "      h1_all = \"\"\n",
        "      for x in range (len(h1)):\n",
        "        if x == len(h1)-1 :\n",
        "          h1_all = h1_all + h1[x].text\n",
        "        else:\n",
        "          h1_all = h1_all + h1[x].text + \". \"\n",
        "\n",
        "      paragraphs_all = \"\"\n",
        "      paragraphs = soup.find_all('p')\n",
        "      for x in range (len(paragraphs)):\n",
        "        if x == len(paragraphs)-1:\n",
        "          paragraphs_all = paragraphs_all + paragraphs[x].text\n",
        "        else:\n",
        "          paragraphs_all = paragraphs_all + paragraphs[x].text + \". \"\n",
        "\n",
        "\n",
        "      h2 = soup.find_all('h2')\n",
        "      h2_all = \"\"\n",
        "      for x in range (len(h2)):\n",
        "        if x == len(h2)-1:\n",
        "          h2_all = h2_all + h2[x].text\n",
        "        else:\n",
        "          h2_all = h2_all + h2[x].text + \". \"\n",
        "\n",
        "\n",
        "      h3 = soup.find_all('h3')\n",
        "      h3_all = \"\"\n",
        "      for x in range (len(h3)):\n",
        "        if x == len(h3)-1:\n",
        "          h3_all = h3_all + h3[x].text\n",
        "        else:\n",
        "          h3_all = h3_all + h3[x].text + \". \"\n",
        "\n",
        "      allthecontent = str(title) + \" \" + str(description) + \" \" + str(h1_all) + \" \" + str(h2_all) + \" \" + str(h3_all) + \" \" + str(paragraphs_all)\n",
        "      allthecontent = str(allthecontent)[0:999]\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    #return {'none':0}\n",
        "\n",
        "  #title path to your Google NLP API credentials\n",
        "  path_credential = \"/content/gdrive/MyDrive/Colab Notebooks/WebsiteCategorization/keyfile.json\"\n",
        "\n",
        "  #We use the Google NLP API module to categorize the websites.\n",
        "  #First, we need to provide the Google Application credentials.\n",
        "  #We make the request to NLP API.\n",
        "  #We print the assigned category and the confidence score.\n",
        "\n",
        "  import os\n",
        "  from google.cloud import language_v1\n",
        "  from google.cloud import language\n",
        "  from google.cloud.language_v1 import types\n",
        "  from google.colab import drive\n",
        "  #from google.cloud.language_v1.types import enums\n",
        "\n",
        "  drive.mount('/content/gdrive')\n",
        "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_credential\n",
        "  # google nlp api's credentials to be created\n",
        "\n",
        "  classifiers = {\"Adult\" : 10,\"Arts & Entertainment\" : 12,\"Autos & Vehicles\" : 2,\"Beauty & Fitness\" : 15,\"Books & Literature\" : 12,\"Business & Industrial\" : 1,\"Computer and Electronics\" : 18,\"Finance\" : 18,\"Food & Drink\" : 20,\"Games\" : 12,\"Health\" : 20,\"Hobbies & Leisure\" : 10,\"Home & Garden\" : 12,\"Internet & Telecom\" : 12,\"Jobs & Education\" : 15,\"Law & Government\" : 10,\"News\" : 8,\"Online Communities\" : 12,\"People & Society\" : 10,\"Pets & Animals\" : 15,\"Real Estate\" : 20,\"Reference\" : 12,\"Science\" : 12,\"Sensitive Subjects\" : 12,\"Shopping\" : 22,\"Sports\" : 10,\"Travel & Transportation\" : 12,\"others\" : 8,}\n",
        "\n",
        "  try:\n",
        "    text_content = str(allthecontent)\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
        "    #document = {\"content\" : text_content , \"type\": type_ , \"language\" : \"en\"}\n",
        "    document = language_v1.Document(\n",
        "          content=text_content,\n",
        "          type=language_v1.Document.Type.PLAIN_TEXT,\n",
        "          language=\"en\"\n",
        "      )\n",
        "    encoding_type = types.EncodingType.UTF8\n",
        "    # Make the API request\n",
        "    response = client.classify_text(document=document)\n",
        "    #response = client.classify_text(document)\n",
        "\n",
        "    result = (response.categories[0].name).split('/')\n",
        "    classification_response = result[1].strip()\n",
        "    classification_confidence = int(float(response.categories[0].confidence)*100)\n",
        "\n",
        "    if(classification_confidence < 15.00):\n",
        "      classification_response = 'others'\n",
        "\n",
        "    print(url + \" categorized under \" + classification_response + \" with \" + str(classification_confidence) + \"% of confidence\")\n",
        "    #print(url + \" categorized under \" + response.categories[0].name + \" with \" + str(int(response.categories[0].confidence,3)*100) +\"% of confidence\")\n",
        "    #print(url + \" categorized under \" + response.categories[0].name + \" with \" + str(int(response.categories[0].confidence, 3) * 100) + \"% of confidence\")\n",
        "    return { classification_response : classifiers[classification_response]}\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    #return {'none': 0}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classify(\"https://instagram.com\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j11rVASOMytt",
        "outputId": "598613d3-011b-4bf4-92b7-07f11ba8432b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cloudscraper in /usr/local/lib/python3.10/dist-packages (1.2.71)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.10/dist-packages (from cloudscraper) (3.1.1)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from cloudscraper) (2.31.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper) (2023.11.17)\n",
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.10/dist-packages (1.1.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: google-cloud-language in /usr/local/lib/python3.10/dist-packages (2.11.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (1.59.3)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (2023.11.17)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language) (0.5.1)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "https://instagram.com categorized under Online Communities with 91% of confidence\n",
            "{'Online Communities': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zU3WZdCmNPTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJSz-SYHkuDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P50Z2xbgkuAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Sw-J08pkt4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SfOzkI9kt2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "import json\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "from google.cloud import language_v1\n",
        "from google.cloud.language_v1 import types\n",
        "\n",
        "class RequestHandler(BaseHTTPRequestHandler):\n",
        "    def do_POST(self):\n",
        "        content_length = int(self.headers['Content-Length'])\n",
        "        payload = self.rfile.read(content_length)\n",
        "        data = json.loads(payload.decode('utf-8'))\n",
        "\n",
        "        try:\n",
        "            url = data.get('url')\n",
        "\n",
        "            # Scraping code\n",
        "            scraper = cloudscraper.create_scraper()\n",
        "            headers = {'user-agent' : 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121'}\n",
        "            r = scraper.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "            # Extracting content from soup\n",
        "            allthecontent = get_website_content(soup)\n",
        "\n",
        "            # Google NLP API classification\n",
        "            classification_response, classification_confidence = classify_content(allthecontent)\n",
        "\n",
        "            result = {\n",
        "                'url': url,\n",
        "                'category': classification_response,\n",
        "                'confidence': classification_confidence\n",
        "            }\n",
        "\n",
        "            self.send_response(200)\n",
        "            self.send_header('Content-type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps(result).encode('utf-8'))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.send_response(500)\n",
        "            self.send_header('Content-type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps({'error': str(e)}).encode('utf-8'))\n",
        "\n",
        "def get_website_content(soup):\n",
        "    # Your code to extract content from soup\n",
        "    # Example: Extracting title and paragraphs\n",
        "    title = soup.find('title').text\n",
        "    paragraphs = soup.find_all('p')\n",
        "    paragraphs_all = \" \".join([p.text for p in paragraphs])\n",
        "    return f\"{title} {paragraphs_all}\"\n",
        "\n",
        "def classify_content(allthecontent):\n",
        "    # Your code for Google NLP API classification\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
        "    document = language_v1.Document(\n",
        "        content=allthecontent,\n",
        "        type=type_,\n",
        "        language=\"en\"\n",
        "    )\n",
        "    encoding_type = types.EncodingType.UTF8\n",
        "    response = client.classify_text(document=document)\n",
        "\n",
        "    # Extracting category and confidence\n",
        "    result = (response.categories[0].name).split('/')\n",
        "    classification_response = result[1].strip()\n",
        "    classification_confidence = int(float(response.categories[0].confidence) * 100)\n",
        "\n",
        "    if classification_confidence < 15.00:\n",
        "        classification_response = 'others'\n",
        "\n",
        "    return classification_response, classification_confidence\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    server_address = ('', 8000)\n",
        "    httpd = HTTPServer(server_address, RequestHandler)\n",
        "    print('Starting server on port 8000...')\n",
        "    httpd.serve_forever()\n"
      ],
      "metadata": {
        "id": "PyuPlKaEktzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKopg-qTktwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_OLuXzokttE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xtk8F2UTktpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "import json\n",
        "import cloudscraper\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from google.cloud import language_v1\n",
        "from google.cloud import language\n",
        "from google.cloud.language_v1 import types\n",
        "from google.colab import drive\n",
        "\n",
        "class RequestHandler(BaseHTTPRequestHandler):\n",
        "    def do_POST(self):\n",
        "        content_length = int(self.headers['Content-Length'])\n",
        "        payload = self.rfile.read(content_length)\n",
        "        data = json.loads(payload.decode('utf-8'))\n",
        "\n",
        "        try:\n",
        "            url = data.get('url')\n",
        "\n",
        "            # code for scraping\n",
        "            scraper = cloudscraper.create_scraper()\n",
        "            headers = {'user-agent' : 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121'}\n",
        "\n",
        "            r = scraper.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "            # Your existing code for classification\n",
        "            allthecontent = get_website_content(soup)\n",
        "\n",
        "            # Your existing code for Google NLP API classification\n",
        "            classification_response, points = classify_content(allthecontent)\n",
        "\n",
        "            result = {\n",
        "                'url': url,\n",
        "                'category': classification_response,\n",
        "                'points': points\n",
        "            }\n",
        "\n",
        "            self.send_response(200)\n",
        "            self.send_header('Content-type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps(result).encode('utf-8'))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.send_response(500)\n",
        "            self.send_header('Content-type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps({'error': str(e)}).encode('utf-8'))\n",
        "\n",
        "def get_website_content(soup):\n",
        "    title = soup.find('title').text\n",
        "    description = soup.find('meta', attrs={'name':'description'})\n",
        "\n",
        "    if \"content\" in str(description):\n",
        "      description = description.get(\"content\")\n",
        "    else:\n",
        "      description = \"\"\n",
        "\n",
        "    h1 = soup.find_all('h1')\n",
        "    h1_all = \"\"\n",
        "    for x in range (len(h1)):\n",
        "      if x == len(h1)-1 :\n",
        "        h1_all = h1_all + h1[x].text\n",
        "      else:\n",
        "        h1_all = h1_all + h1[x].text + \". \"\n",
        "\n",
        "    paragraphs_all = \"\"\n",
        "    paragraphs = soup.find_all('p')\n",
        "    for x in range (len(paragraphs)):\n",
        "      if x == len(paragraphs)-1:\n",
        "        paragraphs_all = paragraphs_all + paragraphs[x].text\n",
        "      else:\n",
        "        paragraphs_all = paragraphs_all + paragraphs[x].text + \". \"\n",
        "\n",
        "\n",
        "    h2 = soup.find_all('h2')\n",
        "    h2_all = \"\"\n",
        "    for x in range (len(h2)):\n",
        "      if x == len(h2)-1:\n",
        "        h2_all = h2_all + h2[x].text\n",
        "      else:\n",
        "        h2_all = h2_all + h2[x].text + \". \"\n",
        "\n",
        "\n",
        "    h3 = soup.find_all('h3')\n",
        "    h3_all = \"\"\n",
        "    for x in range (len(h3)):\n",
        "      if x == len(h3)-1:\n",
        "        h3_all = h3_all + h3[x].text\n",
        "      else:\n",
        "        h3_all = h3_all + h3[x].text + \". \"\n",
        "\n",
        "    allthecontent = str(title) + \" \" + str(description) + \" \" + str(h1_all) + \" \" + str(h2_all) + \" \" + str(h3_all) + \" \" + str(paragraphs_all)\n",
        "    allthecontent = str(allthecontent)[0:999]\n",
        "\n",
        "def classify_content(allthecontent):\n",
        "    path_credential = \"/content/gdrive/MyDrive/Colab Notebooks/WebsiteCategorization/keyfile.json\"\n",
        "    drive.mount('/content/gdrive')\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_credential\n",
        "    classifiers = {\"Adult\" : 10,\"Arts & Entertainment\" : 12,\"Autos & Vehicles\" : 2,\"Beauty & Fitness\" : 15,\"Books & Literature\" : 12,\"Business & Industrial\" : 1,\"Computer and Electronics\" : 18,\"Finance\" : 18,\"Food & Drink\" : 20,\"Games\" : 12,\"Health\" : 20,\"Hobbies & Leisure\" : 10,\"Home & Garden\" : 12,\"Internet & Telecom\" : 12,\"Jobs & Education\" : 15,\"Law & Government\" : 10,\"News\" : 8,\"Online Communities\" : 12,\"People & Society\" : 10,\"Pets & Animals\" : 15,\"Real Estate\" : 20,\"Reference\" : 12,\"Science\" : 12,\"Sensitive Subjects\" : 12,\"Shopping\" : 22,\"Sports\" : 10,\"Travel & Transportation\" : 12,\"others\" : 8,}\n",
        "\n",
        "    try:\n",
        "      text_content = str(allthecontent)\n",
        "      client = language_v1.LanguageServiceClient()\n",
        "      type_ = language_v1.Document.Type.PLAIN_TEXT\n",
        "      #document = {\"content\" : text_content , \"type\": type_ , \"language\" : \"en\"}\n",
        "      document = language_v1.Document(\n",
        "            content=text_content,\n",
        "            type=language_v1.Document.Type.PLAIN_TEXT,\n",
        "            language=\"en\"\n",
        "        )\n",
        "      encoding_type = types.EncodingType.UTF8\n",
        "      # Make the API request\n",
        "      response = client.classify_text(document=document)\n",
        "      #response = client.classify_text(document)\n",
        "\n",
        "      print(\" categorized under \" + response.categories[0].name + \" with \" + str(int(float(response.categories[0].confidence)*100)) + \"% of confidence\")\n",
        "      #print(url + \" categorized under \" + response.categories[0].name + \" with \" + str(int(response.categories[0].confidence,3)*100) +\"% of confidence\")\n",
        "      #print(url + \" categorized under \" + response.categories[0].name + \" with \" + str(int(response.categories[0].confidence, 3) * 100) + \"% of confidence\")\n",
        "\n",
        "      # Extracting category and confidence\n",
        "      result = (response.categories[0].name).split('/')\n",
        "      classification_response = result[1].strip()\n",
        "      classification_confidence = int(float(response.categories[0].confidence) * 100)\n",
        "\n",
        "      if classification_confidence < 15.00:\n",
        "          classification_response = 'others'\n",
        "\n",
        "      return classification_response, classifiers[classification_response]\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return 'none', 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    server_address = ('', 8000)\n",
        "    httpd = HTTPServer(server_address, RequestHandler)\n",
        "    print('Starting server on port 8000...')\n",
        "    #httpd.serve_forever()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJUtnFN_ktlj",
        "outputId": "26bad88f-f99c-4ed2-9f8a-a5ad2b6cc434"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server on port 8000...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzibJa2Ao1-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}